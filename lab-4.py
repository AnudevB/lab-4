# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IdryBYsLDZfplz5WdnX8V0X9Au7WrpsR
"""

import tensorflow as tf    # Imports the TensorFlow library
import matplotlib.pyplot as plt   #Imports the Matplotlib library

mnist = tf.keras.datasets.mnist    #Loads the MNIST dataset from TensorFlow, which consists of handwritten digits.
(x_train, y_train), (x_test, y_test) = mnist.load_data()  #splits the MNIST dataset into training and testing data sets. 
x_train, x_test = x_train / 255.0, x_test / 255.0    # Normalizes the pixel values of the images between 0 and 1 by dividing them by 255

plt.figure()
plt.imshow(x_test[1])   # Displays the image at index 1 from the testing set
plt.colorbar()   # Adds a colorbar to the plot, representing the pixel intensity values.
plt.grid(False)   #Disables grid lines on the plot
plt.show()   # Displays the plot.

model = tf.keras.models.Sequential([   # Creates a sequential model using Keras
    tf.keras.layers.Flatten(input_shape=(28, 28)),   #It reshapes the input data from a 2D array  into a 1D array 
    tf.keras.layers.Dense(128, activation='relu'),  # Adds a dense layer with 128 units to the model,the activation function used  is ReLU
    tf.keras.layers.Dropout(0.2),    #Dropout is a regularization technique that randomly sets a fraction of the input units to 0 at each update during training
    tf.keras.layers.Dense(10)   # Adds another dense layer to the model with 10 units(no activation function therefore  this layer will output raw, unnormalized scores )
])

prediction1 = model(x_train[:1]).numpy() #To predict the output for the first image in the training set and converts the result to a NumPy array.
print(prediction1)
predictions1 = model(x_test[:1]).numpy()  # Predict the output and converts the result to an array
print(predictions1)
unmodel = tf.nn.softmax(predictions1).numpy()  # Applies the softmax activation function to the predicted outputs and converts the result to a NumPy array
print(unmodel[0])
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)   # Creates a loss function object for sparse categorical cross-entropy
loss = loss_fn(y_train[:1], prediction1).numpy()   # Computes the loss value between the true labels of the first training image 
model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])   # Configures the model for training with the optimizer as Adam and the loss function as loss_fn
model.fit(x_train, y_train, epochs=5)    # Trains the model for 5 epochs
model.evaluate(x_test, y_test, verbose=2)    # Evaluates the trained model on the testing data
probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])  # Creates a new sequential model that consists of the trained model and an additional softmax layer to convert  output into class probabilities.
print(probability_model(x_test[:5]))  #Uses the probability model to predict class probabilities for the first five testing images and prints the results.